{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "# Object Detection Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This demo shows how a smart video IoT solution may be created using Intel® hardware and software\n",
    "tools to perform object detection. This solution detects any number of objects within a video frame\n",
    "looking specifically for known objects.\n",
    "\n",
    "The results for each frame are stored in a text file that is later read by a second pass to annotate\n",
    "the input video with boxes around detected objects with a label and probability value.\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "This demo includes an example for the following:\n",
    "\n",
    "- Application:\n",
    "  - Video and image input is supported using OpenCV\n",
    "  - OpenCV is used to draw bounding boxes around detected objects, labels, and other information\n",
    "  - Visualization of the resulting bounding boxes in the output\n",
    "  - Uses the\n",
    "    [Async API](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Intro_to_Performance.html)\n",
    "    feature of the Inference Engine\n",
    "- Intel® DevCloud for the Edge:\n",
    "  - Submitting inference as jobs that are performed on different edge compute nodes (rather than on\n",
    "    the development node hosting this Jupyter\\* notebook)\n",
    "  - Monitoring job status\n",
    "  - Viewing results and assessing performance for hardware on different compute nodes\n",
    "- [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/openvino-toolkit):\n",
    "  - Create the necessary Intermediate Representation (IR) files for the inference model using the\n",
    "    [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) and\n",
    "    [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "  - Run an inference application on multiple hardware devices using the\n",
    "    [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection application\n",
    "\n",
    "The object detection application uses the Intel® Distribution of OpenVINO™ toolkit to perform\n",
    "inference on an input video to locate known objects within each frame. We will setup, run, and view\n",
    "the results for this application for several different hardware devices (CPU. GPU, etc.) available\n",
    "on the compute nodes within the Intel® DevCloud for the Edge. To accomplish this, we will be\n",
    "performing the following tasks:\n",
    "\n",
    "1. Use the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html)\n",
    "   to download the inference model IR files needed to perform inference\n",
    "2. Use the\n",
    "   [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "   to create the model IR files in the necessary precisions\n",
    "3. Create the job file used to submit running inference on compute nodes\n",
    "4. Submit jobs for different compute nodes and monitor the job status until complete\n",
    "5. View results and assess performance\n",
    "\n",
    "### How it works\n",
    "\n",
    "At startup the object detection application configures itself by parsing the command line arguments\n",
    "and reading the specified labels file. Once configured, the application loads the specified\n",
    "inference model's IR files into the\n",
    "[Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)\n",
    "and runs the three phases described in the following sections.\n",
    "\n",
    "#### Phase 1:  Pre-processing\n",
    "\n",
    "The frames from the specified input video are read and pre-processed to have the frame data ready\n",
    "for input into inference. Each frame is resized and its channels are transposed to match the input\n",
    "of the input requirements of the inference model. The resulting blocks of frame data are written to\n",
    "the binary file `tmp/processed_vid.bin`.\n",
    "\n",
    "#### Phase 2:  Inference\n",
    "\n",
    "The binary file `tmp/processed_vid.bin` is read for input to inference to detect known objects. For\n",
    "each detected object within a frame, a message indicating the object's location, label, etc. is\n",
    "written as a line to the output text file.\n",
    "\n",
    "#### Phase 3:  Post-processing\n",
    "\n",
    "After the object detection application has finished running inference, a second executable is run to\n",
    "annotate the input video with the results from the output text file. The final results is an output\n",
    "video with boxes drawn around detected objects with each box appearing with a label and probability\n",
    "value.\n",
    "\n",
    "In this sample, we provide three performance numbers for each architecture. However, they are not\n",
    "hard limits on the solution's performance. It is important to understand that for any application,\n",
    "you may want to combine preprocessing, inference, and post-processing, as opposed to separating them\n",
    "as done here. Combined preprocessing has several advantages:\n",
    "\n",
    "- If inference is run asynchronously on the accelerator, the rest of the system is available for\n",
    "  parallel tasks for capturing input, preprocessing future frames, or post-processing past frames.\n",
    "- The inference application pipeline has better data locality, allowing for the reuse of data in\n",
    "  caches such as in the application's memory or in the hard drive's cache.\n",
    "\n",
    "To run the application on the Intel® DevCloud for the Edge, a job is submitted to an edge compute\n",
    "node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Neural Compute\n",
    "Stick 2. After inference on the input is completed, the output is stored in the appropriate\n",
    "`results/<JobNum>/` directory. The results are then viewed within this Jupyter\\* Notebook using the\n",
    "`videoHTML` video playback utility.\n",
    "\n",
    "The application and inference code for this sample is already implemented in the files:\n",
    "\n",
    "- First pass, detection using inference: [`object_detection.py`](./object_detection.py)\n",
    "- Second pass, annotation using detection results:\n",
    "  [`object_detection_annotate.py`](object_detection_annotate.py)\n",
    "\n",
    "The following sections will guide you through configuring and running the object detection demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "We begin by importing all the Python\\* modules that will be used within this Jupyter\\* Notebook to\n",
    "run and display the results of the object detection application on the Intel® DevCloud for the Edge:\n",
    "\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) - Operating system specific module (used\n",
    "  for file name parsing)\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) - Time tracking module (used for\n",
    "  measuring execution time)\n",
    "- [matplotlib.pyplot](https://matplotlib.org/) - pyplot is used for displaying output images\n",
    "- [sys](https://docs.python.org/3/library/sys.html#module-sys) - System specific parameters and\n",
    "  functions\n",
    "- [qarpo.demoutils](https://github.com/ColfaxResearch/qarpo) - Provides utilities for displaying\n",
    "  results and managing jobs from within this Jupyter\\* Notebook\n",
    "\n",
    "Run the following cell to import the Python\\* dependencies needed.\n",
    "\n",
    "<br><div class=tip><b>Tip: </b>Select a cell and then use **Ctrl+Enter** to run that cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Python modules successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from qarpo.demoutils import liveQstat, progressIndicator, summaryPlot, videoHTML\n",
    "from qarpo.model_visualizer_link import showModelVisualizerLink\n",
    "\n",
    "print(\"Imported Python modules successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the IR files for the inference model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the\n",
    "[Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "used to convert and optimize trained models into the Intermediate Representation (IR) model files,\n",
    "and the\n",
    "[Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)\n",
    "that uses the IR model files to run inference on hardware devices. The IR model files can be created\n",
    "from trained models from popular frameworks (e.g. Caffe\\*, Tensorflow\\*, etc.). The Intel®\n",
    "Distribution of OpenVINO™ toolkit also includes the\n",
    "[Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) utility to\n",
    "download some common inference models from the\n",
    "[Open Model Zoo](https://github.com/opencv/open_model_zoo).\n",
    "\n",
    "Run the following cell to run the Model Downloader utility with the `--print_all` argument to see\n",
    "all the available inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sphereface\n",
      "aclnet\n",
      "aclnet-int8\n",
      "action-recognition-0001\n",
      "age-gender-recognition-retail-0013\n",
      "alexnet\n",
      "anti-spoof-mn3\n",
      "asl-recognition-0004\n",
      "background-matting-mobilenetv2\n",
      "bert-base-ner\n",
      "bert-large-uncased-whole-word-masking-squad-0001\n",
      "bert-large-uncased-whole-word-masking-squad-emb-0001\n",
      "bert-large-uncased-whole-word-masking-squad-int8-0001\n",
      "bert-small-uncased-whole-word-masking-squad-0001\n",
      "bert-small-uncased-whole-word-masking-squad-0002\n",
      "bert-small-uncased-whole-word-masking-squad-emb-int8-0001\n",
      "bert-small-uncased-whole-word-masking-squad-int8-0002\n",
      "brain-tumor-segmentation-0001\n",
      "brain-tumor-segmentation-0002\n",
      "caffenet\n",
      "cocosnet\n",
      "colorization-siggraph\n",
      "colorization-v2\n",
      "common-sign-language-0001\n",
      "common-sign-language-0002\n",
      "convnext-tiny\n",
      "ctdet_coco_dlav0_512\n",
      "ctpn\n",
      "deblurgan-v2\n",
      "deeplabv3\n",
      "densenet-121\n",
      "densenet-121-tf\n",
      "detr-resnet50\n",
      "dla-34\n",
      "driver-action-recognition-adas-0002\n",
      "drn-d-38\n",
      "efficientdet-d0-tf\n",
      "efficientdet-d1-tf\n",
      "efficientnet-b0\n",
      "efficientnet-b0-pytorch\n",
      "efficientnet-v2-b0\n",
      "efficientnet-v2-s\n",
      "emotions-recognition-retail-0003\n",
      "f3net\n",
      "face-detection-0200\n",
      "face-detection-0202\n",
      "face-detection-0204\n",
      "face-detection-0205\n",
      "face-detection-0206\n",
      "face-detection-adas-0001\n",
      "face-detection-retail-0004\n",
      "face-detection-retail-0005\n",
      "face-detection-retail-0044\n",
      "face-recognition-resnet100-arcface-onnx\n",
      "face-reidentification-retail-0095\n",
      "faceboxes-pytorch\n",
      "facenet-20180408-102900\n",
      "facial-landmarks-35-adas-0002\n",
      "facial-landmarks-98-detection-0001\n",
      "fast-neural-style-mosaic-onnx\n",
      "faster-rcnn-resnet101-coco-sparse-60-0001\n",
      "faster_rcnn_inception_resnet_v2_atrous_coco\n",
      "faster_rcnn_resnet50_coco\n",
      "fastseg-large\n",
      "fastseg-small\n",
      "fbcnn\n",
      "fcrn-dp-nyu-depth-v2-tf\n",
      "formula-recognition-medium-scan-0001\n",
      "formula-recognition-polynomials-handwritten-0001\n",
      "forward-tacotron\n",
      "gaze-estimation-adas-0002\n",
      "gmcnn-places2-tf\n",
      "googlenet-v1\n",
      "googlenet-v1-tf\n",
      "googlenet-v2\n",
      "googlenet-v2-tf\n",
      "googlenet-v3\n",
      "googlenet-v3-pytorch\n",
      "googlenet-v4-tf\n",
      "gpt-2\n",
      "handwritten-english-recognition-0001\n",
      "handwritten-japanese-recognition-0001\n",
      "handwritten-score-recognition-0003\n",
      "handwritten-simplified-chinese-recognition-0001\n",
      "hbonet-0.25\n",
      "hbonet-1.0\n",
      "head-pose-estimation-adas-0001\n",
      "higher-hrnet-w32-human-pose-estimation\n",
      "horizontal-text-detection-0001\n",
      "hrnet-v2-c1-segmentation\n",
      "human-pose-estimation-0001\n",
      "human-pose-estimation-0005\n",
      "human-pose-estimation-0006\n",
      "human-pose-estimation-0007\n",
      "human-pose-estimation-3d-0001\n",
      "hybrid-cs-model-mri\n",
      "i3d-rgb-tf\n",
      "icnet-camvid-ava-0001\n",
      "icnet-camvid-ava-sparse-30-0001\n",
      "icnet-camvid-ava-sparse-60-0001\n",
      "image-retrieval-0001\n",
      "inception-resnet-v2-tf\n",
      "instance-segmentation-person-0007\n",
      "instance-segmentation-security-0002\n",
      "instance-segmentation-security-0091\n",
      "instance-segmentation-security-0228\n",
      "instance-segmentation-security-1039\n",
      "instance-segmentation-security-1040\n",
      "landmarks-regression-retail-0009\n",
      "levit-128s\n",
      "license-plate-recognition-barrier-0001\n",
      "license-plate-recognition-barrier-0007\n",
      "machine-translation-nar-de-en-0002\n",
      "machine-translation-nar-en-de-0002\n",
      "machine-translation-nar-en-ru-0002\n",
      "machine-translation-nar-ru-en-0002\n",
      "mask_rcnn_inception_resnet_v2_atrous_coco\n",
      "mask_rcnn_resnet50_atrous_coco\n",
      "midasnet\n",
      "mixnet-l\n",
      "mobilefacedet-v1-mxnet\n",
      "mobilenet-ssd\n",
      "mobilenet-v1-0.25-128\n",
      "mobilenet-v1-1.0-224\n",
      "mobilenet-v1-1.0-224-tf\n",
      "mobilenet-v2\n",
      "mobilenet-v2-1.0-224\n",
      "mobilenet-v2-1.4-224\n",
      "mobilenet-v2-pytorch\n",
      "mobilenet-v3-large-1.0-224-paddle\n",
      "mobilenet-v3-large-1.0-224-tf\n",
      "mobilenet-v3-small-1.0-224-paddle\n",
      "mobilenet-v3-small-1.0-224-tf\n",
      "mobilenet-yolo-v4-syg\n",
      "modnet-photographic-portrait-matting\n",
      "modnet-webcam-portrait-matting\n",
      "mozilla-deepspeech-0.6.1\n",
      "mozilla-deepspeech-0.8.2\n",
      "mtcnn\n",
      "nanodet-m-1.5x-416\n",
      "nanodet-plus-m-1.5x-416\n",
      "netvlad-tf\n",
      "nfnet-f0\n",
      "noise-suppression-denseunet-ll-0001\n",
      "noise-suppression-poconetlike-0001\n",
      "ocrnet-hrnet-w48-paddle\n",
      "octave-resnet-26-0.25\n",
      "open-closed-eye-0001\n",
      "pedestrian-and-vehicle-detector-adas-0001\n",
      "pedestrian-detection-adas-0002\n",
      "pelee-coco\n",
      "person-attributes-recognition-crossroad-0230\n",
      "person-attributes-recognition-crossroad-0234\n",
      "person-attributes-recognition-crossroad-0238\n",
      "person-detection-0106\n",
      "person-detection-0200\n",
      "person-detection-0201\n",
      "person-detection-0202\n",
      "person-detection-0203\n",
      "person-detection-0301\n",
      "person-detection-0302\n",
      "person-detection-0303\n",
      "person-detection-action-recognition-0005\n",
      "person-detection-action-recognition-0006\n",
      "person-detection-action-recognition-teacher-0002\n",
      "person-detection-asl-0001\n",
      "person-detection-raisinghand-recognition-0001\n",
      "person-detection-retail-0002\n",
      "person-detection-retail-0013\n",
      "person-reidentification-retail-0277\n",
      "person-reidentification-retail-0286\n",
      "person-reidentification-retail-0287\n",
      "person-reidentification-retail-0288\n",
      "person-vehicle-bike-detection-2000\n",
      "person-vehicle-bike-detection-2001\n",
      "person-vehicle-bike-detection-2002\n",
      "person-vehicle-bike-detection-2003\n",
      "person-vehicle-bike-detection-2004\n",
      "person-vehicle-bike-detection-crossroad-0078\n",
      "person-vehicle-bike-detection-crossroad-1016\n",
      "person-vehicle-bike-detection-crossroad-yolov3-1020\n",
      "product-detection-0001\n",
      "pspnet-pytorch\n",
      "quartznet-15x5-en\n",
      "regnetx-3.2gf\n",
      "repvgg-a0\n",
      "repvgg-b1\n",
      "repvgg-b3\n",
      "resnest-50-pytorch\n",
      "resnet-18-pytorch\n",
      "resnet-34-pytorch\n",
      "resnet-50-pytorch\n",
      "resnet-50-tf\n",
      "resnet18-xnor-binary-onnx-0001\n",
      "resnet50-binary-0001\n",
      "retinaface-resnet50-pytorch\n",
      "retinanet-tf\n",
      "rexnet-v1-x1.0\n",
      "rfcn-resnet101-coco-tf\n",
      "road-segmentation-adas-0001\n",
      "robust-video-matting-mobilenetv3\n",
      "se-inception\n",
      "se-resnet-50\n",
      "se-resnext-50\n",
      "semantic-segmentation-adas-0001\n",
      "shufflenet-v2-x0.5\n",
      "shufflenet-v2-x1.0\n",
      "single-human-pose-estimation-0001\n",
      "single-image-super-resolution-1032\n",
      "single-image-super-resolution-1033\n",
      "smartlab-object-detection-0001\n",
      "smartlab-object-detection-0002\n",
      "smartlab-object-detection-0003\n",
      "smartlab-object-detection-0004\n",
      "smartlab-sequence-modelling-0001\n",
      "squeezenet1.0\n",
      "squeezenet1.1\n",
      "ssd-resnet34-1200-onnx\n",
      "ssd300\n",
      "ssd512\n",
      "ssd_mobilenet_v1_coco\n",
      "ssd_mobilenet_v1_fpn_coco\n",
      "ssdlite_mobilenet_v2\n",
      "swin-tiny-patch4-window7-224\n",
      "t2t-vit-14\n",
      "text-detection-0003\n",
      "text-detection-0004\n",
      "text-image-super-resolution-0001\n",
      "text-recognition-0012\n",
      "text-recognition-0014\n",
      "text-recognition-0015\n",
      "text-recognition-0016\n",
      "text-recognition-resnet-fc\n",
      "text-spotting-0005\n",
      "text-to-speech-en-0001\n",
      "text-to-speech-en-multi-0001\n",
      "time-series-forecasting-electricity-0001\n",
      "ultra-lightweight-face-detection-rfb-320\n",
      "ultra-lightweight-face-detection-slim-320\n",
      "unet-camvid-onnx-0001\n",
      "vehicle-attributes-recognition-barrier-0039\n",
      "vehicle-attributes-recognition-barrier-0042\n",
      "vehicle-detection-0200\n",
      "vehicle-detection-0201\n",
      "vehicle-detection-0202\n",
      "vehicle-detection-adas-0002\n",
      "vehicle-license-plate-detection-barrier-0106\n",
      "vehicle-license-plate-detection-barrier-0123\n",
      "vehicle-reid-0001\n",
      "vgg16\n",
      "vgg19\n",
      "vitstr-small-patch16-224\n",
      "wav2vec2-base\n",
      "wavernn\n",
      "weld-porosity-detection-0001\n",
      "yolact-resnet50-fpn-pytorch\n",
      "yolo-v1-tiny-tf\n",
      "yolo-v2-ava-0001\n",
      "yolo-v2-ava-sparse-35-0001\n",
      "yolo-v2-ava-sparse-70-0001\n",
      "yolo-v2-tf\n",
      "yolo-v2-tiny-ava-0001\n",
      "yolo-v2-tiny-ava-sparse-30-0001\n",
      "yolo-v2-tiny-ava-sparse-60-0001\n",
      "yolo-v2-tiny-tf\n",
      "yolo-v2-tiny-vehicle-detection-0001\n",
      "yolo-v3-onnx\n",
      "yolo-v3-tf\n",
      "yolo-v3-tiny-onnx\n",
      "yolo-v3-tiny-tf\n",
      "yolo-v4-tf\n",
      "yolo-v4-tiny-tf\n",
      "yolof\n",
      "yolox-tiny\n"
     ]
    }
   ],
   "source": [
    "!omz_downloader --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=tip><i><b>Tip: </b>The '!' at the beginning is a special Jupyter\\* Notebook command\n",
    "that allows you to run shell commands as if you are at a command line. The above command will also\n",
    "work in a terminal (with the '!' removed).</i></div>\n",
    "\n",
    "Some of these downloaded models are already in the IR format, while others will require the Model\n",
    "Optimizer to be run. For our application, we will be using the\n",
    "[`mobilenet-ssd`](https://github.com/opencv/open_model_zoo/tree/master/models/public/mobilenet-ssd)\n",
    "inference model, which will require being optimization using the Model Optimizer to create the model\n",
    "in the necessary IR format neede by the Inference Engine to run.\n",
    "\n",
    "The format for the Model Downloader command to download a model is:\n",
    "\n",
    "```bash\n",
    "/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py \\\n",
    "    --name <model_name> -o <output_directory>\n",
    "```\n",
    "\n",
    "The input arguments are as follows:\n",
    "\n",
    "- **--name** : The name of the model you want to download. It should be one of the models listed in\n",
    "  the previous cell.\n",
    "- **-o** : The output directory where to store the downloaded model. If the directory does not\n",
    "  exist, it will be created.\n",
    "\n",
    "Run the following cell to download the `mobilenet-ssd` model to the `./raw_models` directory\n",
    "relative to the location of this Jupyter\\* Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nAll files that were downloaded:\n",
      "./raw_models\n",
      "./raw_models/public\n",
      "./raw_models/public/mobilenet-ssd\n",
      "./raw_models/public/mobilenet-ssd/mobilenet-ssd.prototxt\n",
      "./raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel\n"
     ]
    }
   ],
   "source": [
    "#!downloader.py --name mobilenet-ssd -o raw_models\n",
    "!mkdir -p raw_models/public\n",
    "!cp -r  /data/reference-sample-data/raw_models/mobilenet-ssd raw_models/public\n",
    "!echo \"\\nAll files that were downloaded:\"\n",
    "!find ./raw_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above from the output of the last `!find...` command, the necessary model files created\n",
    "using the Caffe\\* framework have been downloaded:\n",
    "\n",
    "- **mobilenet-ssd.prototxt** - The deployed\n",
    "  [Protocol Buffer](https://developers.google.com/protocol-buffers) file that describes the network\n",
    "  architecture to run inference\n",
    "- **mobilenet-ssd.caffemodel** - Binary containing trained weights\n",
    "\n",
    "These files will need to be optimized using the Model Optimizer to create the necessary IR files. We\n",
    "will be running the inference model on different hardware devices which have different requirements\n",
    "on the precision of the model (see\n",
    "[Inference Engine Supported Model Formats](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html#supported_model_formats)\n",
    "for details). For our purposes, we will focus on the use of the two most common precisions, FP32 and\n",
    "FP16.\n",
    "\n",
    "For this model, we will run the Model Optimizer using the format:\n",
    "\n",
    "```bash\n",
    "/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "    --input_model <path_to_caffemodel> \\\n",
    "    --data_type <data_precision> \\\n",
    "    --output_dir <path_to_output_directory> \\\n",
    "    --scale <scale_value> \\\n",
    "    --mean_values [<channel_mean_values>] \n",
    "```\n",
    "\n",
    "The input arguments are as follows:\n",
    "\n",
    "- **--input_model** : The model's input *.caffemodel* file (the *.prototxt* with the same base name\n",
    "  will be automatically found, otherwise the `--input_proto` argument would need to be specified)\n",
    "- **--data_type** : The model's data type and precision (e.g. FP16, FP32, INT8, etc.)\n",
    "- **--output_dir** : Output directory where to store the generated IR model files\n",
    "- **--scale** : Scaling (divide by) value to apply to input values\n",
    "- **--mean_values** : Mean values (one per channel) to be subtracted from input values before\n",
    "  scaling\n",
    "\n",
    "For converting our model, we will use the typical scaling value `256` and mean values\n",
    "`[127,127,127]` used with Caffe\\* models. The complete command will look like the following:\n",
    "\n",
    "```bash\n",
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "    --input_model raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel \\\n",
    "    --data_type <data_precision> \\\n",
    "    --output_dir models/mobilenet-ssd/<data_precision> \\\n",
    "    --scale 256 \\\n",
    "    --mean_values [127,127,127] \n",
    "```\n",
    "\n",
    "We will run the command twice, first with \\<*data_precision*> set to `FP16` and then `FP32` to get\n",
    "all the IR files we will need to run inference on different devices.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>More information on how to use Model Optimizer to convert\n",
    "Caffe\\* models may be found\n",
    "at:[Converting a Caffe\\* Model](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html)</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to use the Model Optimizer to create the `FP16` and `FP32` model IR files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u178245/Reference-samples/iot-devcloud/openvino-dev-latest/developer-samples/python/object-detection-python/raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel\n",
      "\t- Path for generated IR: \t/home/u178245/Reference-samples/iot-devcloud/openvino-dev-latest/developer-samples/python/object-detection-python/models/mobilenet-ssd/FP16\n",
      "\t- IR output name: \tmobilenet-ssd\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \t[127,127,127]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \t256.0\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/data/venv/openvino_2022.2.0_python3.6_RC2_new/lib/python3.6/site-packages/openvino/tools/mo/utils/../front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/u178245/Reference-samples/iot-devcloud/openvino-dev-latest/developer-samples/python/object-detection-python/raw_models/public/mobilenet-ssd/mobilenet-ssd.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \t/data/venv/openvino_2022.2.0_python3.6_RC2_new/lib/python3.6/site-packages/openvino/tools/mo/utils/../../extensions/front/caffe/CustomLayersMapping.xml\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "OpenVINO runtime found in: \t/data/venv/openvino_2022.2.0_python3.6_RC2_new/lib/python3.6/site-packages/openvino\n",
      "OpenVINO runtime version: \t2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "Model Optimizer version: \t2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tprotobuf: not installed, required: < 4.0.0\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev[caffe]\n",
      "Please expect that Model Optimizer conversion might be slow. You are currently using Python protobuf library implementation. \n",
      "Check that your protobuf package version is aligned with requirements_caffe.txt.\n",
      "\n",
      "\n",
      " For more information please refer to Model Optimizer FAQ, question #80. (https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html?question=80#question-80)\n",
      "/data/venv/openvino_2022.2.0_python3.6_RC2_new/lib/python3.6/site-packages/numpy/lib/function_base.py:792: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, order=order, subok=subok, copy=True)\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/u178245/Reference-samples/iot-devcloud/openvino-dev-latest/developer-samples/python/object-detection-python/models/mobilenet-ssd/FP16/mobilenet-ssd.xml\n",
      "[ SUCCESS ] BIN file: /home/u178245/Reference-samples/iot-devcloud/openvino-dev-latest/developer-samples/python/object-detection-python/models/mobilenet-ssd/FP16/mobilenet-ssd.bin\n",
      "[ SUCCESS ] Total execution time: 25.24 seconds. \n",
      "[ SUCCESS ] Memory consumed: 328 MB. \n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u178245/Reference-samples/iot-devcloud/openvino-dev-latest/developer-samples/python/object-detection-python/raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel\n",
      "\t- Path for generated IR: \t/home/u178245/Reference-samples/iot-devcloud/openvino-dev-latest/developer-samples/python/object-detection-python/models/mobilenet-ssd/FP32\n",
      "\t- IR output name: \tmobilenet-ssd\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \t[127,127,127]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \t256.0\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/data/venv/openvino_2022.2.0_python3.6_RC2_new/lib/python3.6/site-packages/openvino/tools/mo/utils/../front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/u178245/Reference-samples/iot-devcloud/openvino-dev-latest/developer-samples/python/object-detection-python/raw_models/public/mobilenet-ssd/mobilenet-ssd.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \t/data/venv/openvino_2022.2.0_python3.6_RC2_new/lib/python3.6/site-packages/openvino/tools/mo/utils/../../extensions/front/caffe/CustomLayersMapping.xml\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "OpenVINO runtime found in: \t/data/venv/openvino_2022.2.0_python3.6_RC2_new/lib/python3.6/site-packages/openvino\n",
      "OpenVINO runtime version: \t2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "Model Optimizer version: \t2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tprotobuf: not installed, required: < 4.0.0\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev[caffe]\n",
      "Please expect that Model Optimizer conversion might be slow. You are currently using Python protobuf library implementation. \n",
      "Check that your protobuf package version is aligned with requirements_caffe.txt.\n",
      "\n",
      "\n",
      " For more information please refer to Model Optimizer FAQ, question #80. (https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html?question=80#question-80)\n"
     ]
    }
   ],
   "source": [
    "# Create FP16 IR files\n",
    "!mo \\\n",
    "--input_model raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP16 \\\n",
    "--output_dir models/mobilenet-ssd/FP16 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] \n",
    "\n",
    "# Create FP32 IR files\n",
    "!mo \\\n",
    "--input_model raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP32 \\\n",
    "--output_dir models/mobilenet-ssd/FP32 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] \n",
    "\n",
    "# find all resulting IR files\n",
    "!echo \"\\nAll IR files that were created:\"\n",
    "!find ./models -name \"*.xml\" -o -name \"*.bin\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above from the output of the last `!find...` command, the required sets of IR model files\n",
    "(`*.xml` and `*.bin`) have been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: View input without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the original video stream\n",
    "used for inference and this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qarpo.demoutils import videoHTML\n",
    "\n",
    "# create link and adjust video path to be able to display from /data using videoHTML()\n",
    "!ln -sfn /data data\n",
    "videoHTML(\n",
    "    \"Cars Video\", [\"./data/reference-sample-data/object-detection-python/cars_1900.mp4\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the job file\n",
    "\n",
    "We will run inference on several different edge compute nodes present in the Intel® DevCloud for the\n",
    "Edge. We will send work to the edge compute nodes by submitting the corresponding non-interactive\n",
    "jobs into a queue. For each job, we will specify the type of the edge compute server that must be\n",
    "allocated for the job.\n",
    "\n",
    "The job file is a [Bash](https://www.gnu.org/software/bash/) script that serves as a wrapper around\n",
    "the Python\\* executable of our application that will be executed directly on the edge compute node.\n",
    "One purpose of the job file is to simplify running an application on different compute nodes by\n",
    "accepting a few arguments and then performing accordingly any necessary steps before and after\n",
    "running the application executable.\n",
    "\n",
    "For this sample, the job file we will be using is already written for you and appears in the next\n",
    "cell. The job file will be submitted as if it were run from the command line using the following\n",
    "format:\n",
    "\n",
    "```bash\n",
    "object_detection_job.sh <output_directory> <device> <fp_precision> <input_file> <num_reqs>\n",
    "```\n",
    "\n",
    "Where the job file input arguments are:\n",
    "\n",
    "- \\<*output_directory*> - Output directory to use to store output files\n",
    "- \\<*device*> - Hardware device to use (e.g. CPU, GPU, etc.)\n",
    "- \\<*fp_precision*> - Which floating point precision inference model to use (FP32 or FP16)\n",
    "- \\<*input_file*> - Path to input video file\n",
    "- \\<*num_reqs*> - The maximum number of requests to allow while using the Async API\n",
    "\n",
    "Based on the input arguments, the job file will do the following:\n",
    "\n",
    "- Change to the working directory `PBS_O_WORKDIR` where this Jupyter\\* Notebook and other files\n",
    "  appear on the compute node\n",
    "- Create the \\<*output_directory*>\n",
    "- Choose the appropriate inference model IR file for the specified \\<*fp_precision*>\n",
    "- Run the application Python\\* executable with the appropriate command line arguments\n",
    "- Run the application annotator Python\\* executable with the appropriate command line arguments\n",
    "\n",
    "Run the following cell to create the `object_detection_job.sh` job file. The\n",
    "[`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile)\n",
    "line at the top will write the cell contents to the specified job file `object_detection_job.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "# Store input arguments: <output_directory> <device> <fp_precision> <input_file> <num_reqs>\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "INPUT_FILE=$4\n",
    "NUM_REQS=$5\n",
    "\n",
    "echo VENV_PATH=$VENV_PATH\n",
    "echo OPENVINO_RUNTIME=$OPENVINO_RUNTIME\n",
    "echo INPUT_FILE=$INPUT_FILE\n",
    "echo FP_MODEL=$FP_MODEL\n",
    "echo INPUT_TILE=$INPUT_FILE\n",
    "echo NUM_REQS=$NUM_REQS\n",
    "\n",
    "# Follow this order of setting up environment for openVINO 2022.1.0.553\n",
    "echo \"Activating a Python virtual environment from ${VENV_PATH}...\"\n",
    "source ${VENV_PATH}/bin/activate\n",
    "echo \"Activating OpenVINO variables from ${OPENVINO_RUNTIME}...\"\n",
    "source ${OPENVINO_RUNTIME}/setupvars.sh\n",
    "\n",
    "# The default path for the job is the user's home directory,\n",
    "#  change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Make sure that the output directory exists.\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "# Set inference model IR files using specified precision\n",
    "MODELPATH=models/mobilenet-ssd/${FP_MODEL}/mobilenet-ssd.xml\n",
    "LABEL_FILE=./labels.txt\n",
    "\n",
    "# Run the object detection code\n",
    "python3 object_detection.py -m $MODELPATH \\\n",
    "                            -i $INPUT_FILE \\\n",
    "                            -o $OUTPUT_FILE \\\n",
    "                            -d $DEVICE \\\n",
    "                            -nireq $NUM_REQS \\\n",
    "                            --labels $LABEL_FILE\n",
    "\n",
    "# Run the output video annotator code\n",
    "SCALE_FRAME_RATE=1    # scale number or output frames to input frames\n",
    "SCALE_RESOLUTION=0.5  # scale output frame resolution \n",
    "python3 object_detection_annotate.py -i $INPUT_FILE \\\n",
    "                                     -o $OUTPUT_FILE \\\n",
    "                                     -f $SCALE_FRAME_RATE \\\n",
    "                                     -s $SCALE_RESOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the demo\n",
    "\n",
    "Running the next cell will display an interactive user interface allowing you to submit jobs to run\n",
    "the demo on multiple edge compute nodes selected by hardware devices, view the output of each job,\n",
    "and compare performance results across jobs.\n",
    "\n",
    "To run a job:\n",
    "\n",
    "1. Select the desired option in the **Target node** list\n",
    "2. Select the desired device in the **Target architecture** list\n",
    "3. Click the **Submit** button\n",
    "\n",
    "After the **Submit** button is clicked, a tab will appear for the new job with a label in the format\n",
    "\"*status*: *JobID*\". Once the job status appears as \"Done\", the **Display output** button may be\n",
    "clicked to view the output for the job.\n",
    "\n",
    "After one or more jobs are done, the performance results for each job may be plotted by clicking the\n",
    "**Plot results** button. Results for each job will be potted as bar graphs for **inference time**\n",
    "and **frames per second**. Lower values mean better performance for **inference time** and higher\n",
    "values mean better performance for **frames per second**. When comparing results, please keep in\n",
    "mind that some architectures are optimized for highest performance, others for low power or other\n",
    "metrics.\n",
    "\n",
    "Run the next cell to begin the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "\n",
    "import qarpo\n",
    "\n",
    "# load job configurations for demo\n",
    "with open(\"./job_config.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# create and run the user job interface\n",
    "job_interface = qarpo.Interface(data)\n",
    "job_interface.displayUI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor job status\n",
    "\n",
    "To check the status of the jobs that have been submitted, use the `qstat` command. The custom\n",
    "Jupyter\\* Notebook widget `liveQstat()` is provided to display the output of `qstat` with live\n",
    "updates.\n",
    "\n",
    "Run the following cell to display the current job status with periodic updates. You should see the\n",
    "jobs that you have submitted (referenced by the `JobID` that gets displayed right after you submit\n",
    "the jobs in the previous step). There should also be an extra job in the queue named\n",
    "`jupyterhub-singleuser`: this job is your current Jupyter\\* Notebook session which is always\n",
    "running.\n",
    "\n",
    "The `S` column shows the current status of each job:\n",
    "\n",
    "- If the status is `Q`, then the job is queued and waiting for available resources\n",
    "- If the status is `R`, then the job is running\n",
    "- If the job is no longer listed, then the job has completed\n",
    "\n",
    "<br><div class=note><i><b> Note: The amount of time spent in the queue depends on the number of\n",
    "users accessing the requested compute nodes. Once the jobs for this sample application begin to run,\n",
    "they should take from 1 to 5 minutes each to complete. </b></i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "For technical support, please see the\n",
    "[Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)# Next\n",
    "steps\n",
    "\n",
    "- [More Jupyter\\* Notebook Samples](https://software.intel.com/content/www/us/en/develop/tools/devcloud/edge/build/sample-apps.html)\n",
    "  \\- additional sample applications\n",
    "- [Jupyter\\* Notebook Tutorials](https://software.intel.com/content/www/us/en/develop/tools/devcloud/edge/learn/tutorials.html)\n",
    "  \\- sample application Jupyter\\* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit)\n",
    "  \\- learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for\n",
    "  implementing inference on the edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "For technical support, please see the\n",
    "[Intel® DevCloud Forums](developer-samples/python/shopper-gaze-monitor-python/shopper_gaze_monitor.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (OpenVINO 2022.2.0)",
   "language": "python",
   "name": "openvino_2022.2-python36-prod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "251.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
